import os
import sys
import json
import torch
from pathlib import Path
from opencc import OpenCC
from transformers import AutoTokenizer, AutoModel
from app.utils.image_searcher import ImageSearcher
from app.utils.query_wiki import WikiSearcher
from app.utils.ner import Ner
from app.utils.graph_utils import convert_graph_to_triples, search_node_item
from app.utils.ccus_kg_search import CCUSKnowledgeGraphSearcher

# å…¨å±€å˜é‡
model = None
tokenizer = None
init_history = None

# åˆå§‹åŒ–å„ç§å·¥å…·
ner = Ner()
image_searcher = ImageSearcher()
wiki_searcher = WikiSearcher()
ccus_searcher = CCUSKnowledgeGraphSearcher()
cc = OpenCC('t2s')

# æ¨¡å‹è·¯å¾„é…ç½®
MODEL_PATHS = [
    "/root/KnowledgeGraph-based-on-Raw-text-A27-main/KnowledgeGraph-based-on-Raw-text-A27-main/models/chatglm-6b",
    "./models/chatglm-6b",
    "THUDM/chatglm-6b"
]

def predict(user_input, history=None):
    global model, tokenizer, init_history
    if not history:
        history = init_history
    return model.chat(tokenizer, user_input, history)


def stream_predict(user_input, history=None):
    global model, tokenizer, init_history
    if not history:
        history = init_history

    ref = ""

    # 1. CCUSçŸ¥è¯†å›¾è°±æ£€ç´¢
    print(f"ğŸ” CCUSçŸ¥è¯†å›¾è°±æ£€ç´¢: {user_input}")
    knowledge, subgraph = ccus_searcher.search_knowledge(user_input)

    if knowledge:
        kg_info = ccus_searcher.format_knowledge_for_prompt(knowledge)
        if kg_info:
            ref += f"CCUSçŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼š\n{kg_info}\n\n"
            print(f"æ‰¾åˆ°CCUSçŸ¥è¯†: {len(knowledge)} æ¡ç›¸å…³è®°å½•")

    # 2. å®ä½“è¯†åˆ«ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
    graph = subgraph if subgraph["nodes"] else {}
    entities = []

    try:
        entities = ner.get_entities(user_input, etypes=["æŠ€æœ¯", "é¡¹ç›®", "æœºæ„", "åœ°ç†", "æ”¿ç­–", "æ ‡å‡†", "ç»æµ", "è®¾å¤‡", "æŒ‡æ ‡", "ç¯å¢ƒ"])
        print("è¯†åˆ«çš„å®ä½“: ", entities)
    except:
        # å¦‚æœå®ä½“è¯†åˆ«å¤±è´¥ï¼Œä½¿ç”¨CCUSæœç´¢å™¨çš„å®ä½“æå–
        entities = ccus_searcher.extract_entities(user_input)
        print("CCUSå®ä½“æå–: ", entities)

    # 3. ä¼ ç»Ÿä¸‰å…ƒç»„æ£€ç´¢ï¼ˆä½œä¸ºè¡¥å……ï¼‰
    triples = []
    for entity in entities:
        try:
            entity_graph = search_node_item(entity, graph if graph else None)
            if entity_graph:
                triples += convert_graph_to_triples(entity_graph, entity)
        except:
            pass

    if triples:
        triples_str = ""
        for t in triples:
            triples_str += f"({t[0]} {t[1]} {t[2]})ï¼›"
        ref += f"è¡¥å……ä¸‰å…ƒç»„ä¿¡æ¯ï¼š{triples_str}\n\n"

    # 4. å›¾åƒæœç´¢
    image = None
    try:
        image = image_searcher.search(user_input)
    except:
        image = None

    # 5. å¤–éƒ¨çŸ¥è¯†æœç´¢ï¼ˆWikipediaç­‰ï¼‰
    wiki = None
    try:
        for ent in entities + [user_input]:
            wiki = wiki_searcher.search(ent)
            if wiki is not None:
                break

        # å°†Wikipediaæœç´¢åˆ°çš„ç¹ä½“è½¬ä¸ºç®€ä½“
        if wiki:
            ref += f"å¤–éƒ¨çŸ¥è¯†ï¼š{cc.convert(wiki.summary)}\n"
            wiki = {
                "title": cc.convert(wiki.title),
                "summary": cc.convert(wiki.summary),
            }
            print("æ‰¾åˆ°Wikipediaä¿¡æ¯:", wiki["title"])
    except Exception as e:
        print(f"Wikipediaæœç´¢å¤±è´¥: {e}")

    if not wiki:
        wiki = {
            "title": "CCUSæŠ€æœ¯çŸ¥è¯†",
            "summary": "åŸºäºCCUSé¢†åŸŸçŸ¥è¯†å›¾è°±çš„ä¸“ä¸šé—®ç­”",
        }

    if model is not None:
        if ref:
            chat_input = f"\n===å‚è€ƒèµ„æ–™===ï¼š\n{ref}ï¼›\n\næ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š\n{user_input}"
        else:
            chat_input = user_input

        clean_history = []
        for user_input, response in history:
            if "===å‚è€ƒèµ„æ–™===" in user_input:
                user_input = user_input.split("===å‚è€ƒèµ„æ–™===")[0]
            clean_history.append((user_input, response))

        print("chat_input: ", chat_input)
        for response, history in model.stream_chat(tokenizer, chat_input, clean_history):
            updates = {}
            for query, response in history:
                updates["query"] = query
                updates["response"] = response

            result = {
                "history": history,
                "updates": updates,
                "image": image,
                "graph": graph,
                "wiki": wiki
            }
            yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'

    else:
        updates = {
            "query": user_input,
            "response": "æ¨¡å‹åŠ è½½ä¸­ï¼Œè¯·ç¨åå†è¯•"
        }

        result = {
            "history": history,
            "updates": updates,
            "image": image,
            "graph": graph,
            "wiki": wiki
        }
        yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'

# åŠ è½½æ¨¡å‹
def start_model():
    global model, tokenizer, init_history

    print("ğŸ¤– æ­£åœ¨åŠ è½½ChatGLM-6Bæ¨¡å‹...")

    # å°è¯•å„ä¸ªå¯èƒ½çš„æ¨¡å‹è·¯å¾„
    model_path = None
    for path in MODEL_PATHS:
        if os.path.exists(path) or path.startswith("THUDM/"):
            model_path = path
            print(f"æ‰¾åˆ°æ¨¡å‹è·¯å¾„: {path}")
            break

    if not model_path:
        print("âŒ æœªæ‰¾åˆ°ChatGLM-6Bæ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½")
        print("è¿è¡Œ: python download_chatglm.py")
        return False

    try:
        from transformers import AutoTokenizer, AutoModel
        # åŠ è½½tokenizer
        print("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

        # åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹æƒé‡...")
        if torch.cuda.is_available():
            model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()
            print("âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU")
        else:
            model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
            print("âš ï¸ æ¨¡å‹åŠ è½½åˆ°CPU (æ€§èƒ½è¾ƒæ…¢)")

        model.eval()

        # åˆå§‹åŒ–å¯¹è¯å†å²
        pre_prompt = "ä½ å« ChatKGï¼Œæ˜¯ä¸€ä¸ªä¸“ä¸šçš„CCUSï¼ˆç¢³æ•é›†åˆ©ç”¨ä¸å°å­˜ï¼‰é¢†åŸŸçŸ¥è¯†å›¾è°±é—®ç­”æœºå™¨äººã€‚ä½ å¯ä»¥åŸºäºCCUSé¢†åŸŸçŸ¥è¯†å›¾è°±å›ç­”ç›¸å…³æŠ€æœ¯é—®é¢˜ã€‚"
        _, history = predict(pre_prompt, [])
        init_history = history

        print("âœ… ChatGLM-6Bæ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return True

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("å°è¯•è‡ªåŠ¨ä¸‹è½½æ¨¡å‹...")

        # å°è¯•è‡ªåŠ¨ä¸‹è½½
        try:
            from transformers import AutoTokenizer, AutoModel
            print("ä»Hugging Faceä¸‹è½½ChatGLM-6B...")

            tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
            if torch.cuda.is_available():
                model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
            else:
                model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)

            model.eval()

            # ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°
            local_path = "./models/chatglm-6b"
            os.makedirs(local_path, exist_ok=True)
            tokenizer.save_pretrained(local_path)
            model.save_pretrained(local_path)

            print(f"âœ… æ¨¡å‹ä¸‹è½½å¹¶ä¿å­˜åˆ°: {local_path}")

            # åˆå§‹åŒ–å¯¹è¯å†å²
            pre_prompt = "ä½ å« ChatKGï¼Œæ˜¯ä¸€ä¸ªä¸“ä¸šçš„CCUSï¼ˆç¢³æ•é›†åˆ©ç”¨ä¸å°å­˜ï¼‰é¢†åŸŸçŸ¥è¯†å›¾è°±é—®ç­”æœºå™¨äººã€‚ä½ å¯ä»¥åŸºäºCCUSé¢†åŸŸçŸ¥è¯†å›¾è°±å›ç­”ç›¸å…³æŠ€æœ¯é—®é¢˜ã€‚"
            _, history = predict(pre_prompt, [])
            init_history = history

            return True

        except Exception as e2:
            print(f"âŒ è‡ªåŠ¨ä¸‹è½½ä¹Ÿå¤±è´¥: {e2}")
            print("è¯·æ‰‹åŠ¨ä¸‹è½½ChatGLM-6Bæ¨¡å‹")
            return False